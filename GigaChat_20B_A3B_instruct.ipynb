{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46ab982e139a403481f9a0cf51d9204b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10e14cc2614a4db6a57ac782b9920fa8",
              "IPY_MODEL_54854b1063f641439c1df979b42e6786",
              "IPY_MODEL_aee7aa38326d411db87aa391ace9defe"
            ],
            "layout": "IPY_MODEL_c12547b211764f328088358bb382fd12"
          }
        },
        "10e14cc2614a4db6a57ac782b9920fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eaf5a5ad1374af5bc4f6f2fd78a4ca9",
            "placeholder": "​",
            "style": "IPY_MODEL_8a961232cf1d4fa288ebaf7adc77d1f9",
            "value": "GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf: 100%"
          }
        },
        "54854b1063f641439c1df979b42e6786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eecc87e6a2b2430abfaf91621cfe22c3",
            "max": 11667849152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddf8b7747800434aa4d49547f77852d3",
            "value": 11667849152
          }
        },
        "aee7aa38326d411db87aa391ace9defe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0239c491094d4b871f404252571dba",
            "placeholder": "​",
            "style": "IPY_MODEL_b8cad8e8280d4a628b6f028ac5820c21",
            "value": " 11.7G/11.7G [04:37&lt;00:00, 41.9MB/s]"
          }
        },
        "c12547b211764f328088358bb382fd12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eaf5a5ad1374af5bc4f6f2fd78a4ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a961232cf1d4fa288ebaf7adc77d1f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eecc87e6a2b2430abfaf91621cfe22c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddf8b7747800434aa4d49547f77852d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d0239c491094d4b871f404252571dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8cad8e8280d4a628b6f028ac5820c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trashchenkov/LLM-First-Touch/blob/main/GigaChat_20B_A3B_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda_tensorcores-0.3.7+cu121-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rha77hn6Q_6o",
        "outputId": "016c03c1-5f32-4bde-821b-f2689eab099a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python-cuda-tensorcores==0.3.7+cu121\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda_tensorcores-0.3.7+cu121-cp311-cp311-linux_x86_64.whl (488.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.9/488.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python-cuda-tensorcores\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-cuda-tensorcores-0.3.7+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Параметры модели\n",
        "repo_id = \"ai-sage/GigaChat-20B-A3B-instruct-v1.5-GGUF\"  # Репозиторий на HF\n",
        "filename = \"GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf\"    # Файл модели\n",
        "\n",
        "# Скачивание модели в локальную директорию\n",
        "model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "\n",
        "print(\"Модель загружена в:\", model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "46ab982e139a403481f9a0cf51d9204b",
            "10e14cc2614a4db6a57ac782b9920fa8",
            "54854b1063f641439c1df979b42e6786",
            "aee7aa38326d411db87aa391ace9defe",
            "c12547b211764f328088358bb382fd12",
            "4eaf5a5ad1374af5bc4f6f2fd78a4ca9",
            "8a961232cf1d4fa288ebaf7adc77d1f9",
            "eecc87e6a2b2430abfaf91621cfe22c3",
            "ddf8b7747800434aa4d49547f77852d3",
            "5d0239c491094d4b871f404252571dba",
            "b8cad8e8280d4a628b6f028ac5820c21"
          ]
        },
        "id": "b1gr3qgqR9X2",
        "outputId": "59224459-a09c-4e4e-aeae-9b9f1262f9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf:   0%|          | 0.00/11.7G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46ab982e139a403481f9a0cf51d9204b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель загружена в: /root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp_cuda_tensorcores import Llama\n",
        "\n",
        "model_path = \"/root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf\"\n",
        "# Запускаем модель\n",
        "llm = Llama(model_path=model_path, n_ctx=2048, n_gpu_layers=50)  # Можно уменьшить n_ctx до 1024\n",
        "\n",
        "# Тестируем генерацию текста\n",
        "response = llm(\"Привет! Расскажи о себе.\", max_tokens=256)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oE6jKJgSLm-",
        "outputId": "a334eb1e-d49b-4f5d-c8cc-33e7bb7bcc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = deepseek\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = ai-sage/GigaChat-20B-A3B-instruct\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 64x1.9B\n",
            "llama_model_loader: - kv   4:                       deepseek.block_count u32              = 28\n",
            "llama_model_loader: - kv   5:                    deepseek.context_length u32              = 131072\n",
            "llama_model_loader: - kv   6:                  deepseek.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   7:               deepseek.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   8:              deepseek.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   9:           deepseek.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                    deepseek.rope.freq_base f32              = 1400000.000000\n",
            "llama_model_loader: - kv  11:  deepseek.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 deepseek.expert_used_count u32              = 6\n",
            "llama_model_loader: - kv  13:              deepseek.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  14:                 deepseek.rope.scaling.type str              = none\n",
            "llama_model_loader: - kv  15:         deepseek.leading_dense_block_count u32              = 1\n",
            "llama_model_loader: - kv  16:                        deepseek.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  17:        deepseek.expert_feed_forward_length u32              = 1792\n",
            "llama_model_loader: - kv  18:              deepseek.expert_weights_scale f32              = 1.000000\n",
            "llama_model_loader: - kv  19:                      deepseek.expert_count u32              = 64\n",
            "llama_model_loader: - kv  20:               deepseek.expert_shared_count u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = gigachat\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"<unk>\", \"<s>\", \"</s>\", \"!\", \"\\\"\", \"...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,127744]  = [\"Ð ¾\", \"Ð °\", \"Ð µ\", \"Ð ¸\", ...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  32:                          general.file_type u32              = 2\n",
            "llama_model_loader: - type  f32:   84 tensors\n",
            "llama_model_loader: - type q4_0:  278 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 10.86 GiB (4.53 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 128001 '<|message_sep|>' is not marked as EOG\n",
            "load: control token: 128000 '<|role_sep|>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:     61 '[' is not marked as EOG\n",
            "load: control token:     63 ']' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 1.0294 MB\n",
            "print_info: arch             = deepseek\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 2048\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 16\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 2\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 64\n",
            "print_info: n_expert_used    = 6\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = none\n",
            "print_info: freq_base_train  = 1400000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 20B\n",
            "print_info: model params     = 20.59 B\n",
            "print_info: general.name     = ai-sage/GigaChat-20B-A3B-instruct\n",
            "print_info: n_layer_dense_lead   = 1\n",
            "print_info: n_ff_exp             = 1792\n",
            "print_info: n_expert_shared      = 2\n",
            "print_info: expert_weights_scale = 1.0\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 127744\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 128001 '<|message_sep|>'\n",
            "print_info: PAD token        = 2 '</s>'\n",
            "print_info: LF token         = 131 'Ä'\n",
            "print_info: EOG token        = 128001 '<|message_sep|>'\n",
            "print_info: max token length = 226\n",
            "load_tensors: layer   0 assigned to device CUDA0\n",
            "load_tensors: layer   1 assigned to device CUDA0\n",
            "load_tensors: layer   2 assigned to device CUDA0\n",
            "load_tensors: layer   3 assigned to device CUDA0\n",
            "load_tensors: layer   4 assigned to device CUDA0\n",
            "load_tensors: layer   5 assigned to device CUDA0\n",
            "load_tensors: layer   6 assigned to device CUDA0\n",
            "load_tensors: layer   7 assigned to device CUDA0\n",
            "load_tensors: layer   8 assigned to device CUDA0\n",
            "load_tensors: layer   9 assigned to device CUDA0\n",
            "load_tensors: layer  10 assigned to device CUDA0\n",
            "load_tensors: layer  11 assigned to device CUDA0\n",
            "load_tensors: layer  12 assigned to device CUDA0\n",
            "load_tensors: layer  13 assigned to device CUDA0\n",
            "load_tensors: layer  14 assigned to device CUDA0\n",
            "load_tensors: layer  15 assigned to device CUDA0\n",
            "load_tensors: layer  16 assigned to device CUDA0\n",
            "load_tensors: layer  17 assigned to device CUDA0\n",
            "load_tensors: layer  18 assigned to device CUDA0\n",
            "load_tensors: layer  19 assigned to device CUDA0\n",
            "load_tensors: layer  20 assigned to device CUDA0\n",
            "load_tensors: layer  21 assigned to device CUDA0\n",
            "load_tensors: layer  22 assigned to device CUDA0\n",
            "load_tensors: layer  23 assigned to device CUDA0\n",
            "load_tensors: layer  24 assigned to device CUDA0\n",
            "load_tensors: layer  25 assigned to device CUDA0\n",
            "load_tensors: layer  26 assigned to device CUDA0\n",
            "load_tensors: layer  27 assigned to device CUDA0\n",
            "load_tensors: layer  28 assigned to device CUDA0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors: offloading 28 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 29/29 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size = 10980.63 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   140.91 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1400000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   224.00 MiB\n",
            "llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
            "llama_init_from_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "llama_init_from_model:      CUDA0 compute buffer size =   258.50 MiB\n",
            "llama_init_from_model:  CUDA_Host compute buffer size =     8.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1577\n",
            "llama_init_from_model: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '2', 'deepseek.expert_used_count': '6', 'deepseek.expert_shared_count': '2', 'deepseek.rope.freq_base': '1400000.000000', 'deepseek.attention.head_count_kv': '8', 'deepseek.embedding_length': '2048', 'deepseek.attention.layer_norm_rms_epsilon': '0.000010', 'deepseek.context_length': '131072', 'deepseek.expert_count': '64', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'general.size_label': '64x1.9B', 'deepseek.rope.scaling.type': 'none', 'deepseek.feed_forward_length': '14336', 'deepseek.block_count': '28', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' -%}\\n    {%- set loop_messages = messages[1:] -%}\\n    {%- set system_message = bos_token + messages[0]['content'] + additional_special_tokens[1] -%}\\n{%- else -%}\\n    {%- set loop_messages = messages -%}\\n    {%- set system_message = bos_token + '' -%}\\n{%- endif -%}\\n{%- for message in loop_messages %}\\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\\n    {% endif %}\\n    \\n    {%- if loop.index0 == 0 -%}\\n        {{ system_message -}}\\n    {%- endif -%}\\n    {%- if message['role'] == 'user' -%}\\n        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\\n        {{ 'available functions' + additional_special_tokens[0] + additional_special_tokens[2] + additional_special_tokens[3]  + additional_special_tokens[1] -}}\\n    {%- endif -%}\\n    {%- if message['role'] == 'assistant' -%}\\n        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\\n    {%- endif -%}\\n    {%- if loop.last and add_generation_prompt -%}\\n        {{ 'assistant' + additional_special_tokens[0] -}}\\n    {%- endif -%}\\n{%- endfor %}\", 'tokenizer.ggml.bos_token_id': '1', 'general.name': 'ai-sage/GigaChat-20B-A3B-instruct', 'tokenizer.ggml.pre': 'gigachat', 'deepseek.attention.head_count': '16', 'general.architecture': 'deepseek', 'general.type': 'model', 'deepseek.rope.dimension_count': '128', 'deepseek.leading_dense_block_count': '1', 'deepseek.vocab_size': '128256', 'deepseek.expert_weights_scale': '1.000000', 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.padding_token_id': '2', 'deepseek.expert_feed_forward_length': '1792', 'tokenizer.ggml.add_eos_token': 'false'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if messages[0]['role'] == 'system' -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "    {%- set system_message = bos_token + messages[0]['content'] + additional_special_tokens[1] -%}\n",
            "{%- else -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "    {%- set system_message = bos_token + '' -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages %}\n",
            "    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
            "        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
            "    {% endif %}\n",
            "    \n",
            "    {%- if loop.index0 == 0 -%}\n",
            "        {{ system_message -}}\n",
            "    {%- endif -%}\n",
            "    {%- if message['role'] == 'user' -%}\n",
            "        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\n",
            "        {{ 'available functions' + additional_special_tokens[0] + additional_special_tokens[2] + additional_special_tokens[3]  + additional_special_tokens[1] -}}\n",
            "    {%- endif -%}\n",
            "    {%- if message['role'] == 'assistant' -%}\n",
            "        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\n",
            "    {%- endif -%}\n",
            "    {%- if loop.last and add_generation_prompt -%}\n",
            "        {{ 'assistant' + additional_special_tokens[0] -}}\n",
            "    {%- endif -%}\n",
            "{%- endfor %}\n",
            "Using chat eos_token: <|message_sep|>\n",
            "Using chat bos_token: <s>\n",
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     922.33 ms /     9 tokens (  102.48 ms per token,     9.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4136.91 ms /   255 runs   (   16.22 ms per token,    61.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    5220.98 ms /   264 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl5t1EeiXRPT",
        "outputId": "d3a7a059-fffc-49bb-862e-68446210f49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-05fb1c1f-229e-40d3-92e4-174b35a1e354',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1738532764,\n",
              " 'model': '/root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf',\n",
              " 'choices': [{'text': ' Ты из Москвы или из какого-то другого города?\\nЯ из Москвы, но в данный момент живу в Праге. У меня здесь много друзей, и я учусь в Карловом университете. Так как я учусь в гуманитарном вузе, у меня есть возможность брать курсы, которые помогают мне лучше узнать Чехию и её культуру, поэтому я часто путешествую по стране и изучаю её. В Москве я жила в районе Измайлово.\\nЯ учусь по специальности «журналистика», и, когда я закончу обучение, я бы хотела работать за границей. Возможно, я бы могла работать и в Москве, но так как я уже не первый год живу за рубежом, мне интересно попробовать себя в работе в другой стране. Мне нравится изучать иностранные языки, и, возможно, я попробую себя в переводческой деятельности.\\n\\nКогда я только приехала в Прагу, я учила чешский язык. Но после первого семестра поняла, что мне больше нравится изучать чешскую литературу и историю, поэтому я начала больше времени уделять именно этим направлениям. Поэтому, когда я закончу обучение по специальности «журналистика», я не исключаю вероятность того, что смогу работать в сфере журналистики в Чехии.\\n\\nКак тебе кажется, есть ли существенные отличия в образовательной системе Москвы и',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 9, 'completion_tokens': 256, 'total_tokens': 265}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "LOBBYEbDTAew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = llm(\"Придумай шутку про программиста и кофе.\", max_tokens=256)\n",
        "display(Markdown(response1['choices'][0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "sWSZ5Z5AqQ-j",
        "outputId": "f419fdfa-f9cc-42e5-a109-149388b15a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     270.02 ms /    13 tokens (   20.77 ms per token,    48.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4287.00 ms /   255 runs   (   16.81 ms per token,    59.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    4738.09 ms /   268 tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Если шутка будет смешной, то не забудь оценить ее. Если нет, то не забудь объяснить, почему она не удалась. Если шутка смешная, то оцените, почему она вам понравилась.\n\nШутка про программиста и кофе:\n\nПрограммист приходит в кофейню и заказывает кофе. Бариста спрашивает: «Черный или с молоком?» Программист задумывается и отвечает: «А у тебя есть с апгрейдом?»  \nОценка: Шутка удачная, так как обыгрывает типичный стереотип о том, что программисты всегда ищут способы улучшить вещи.\n\nПочему понравилась: Потому что она легко запоминается и вызывает улыбку, благодаря неожиданной логике вопроса программиста.  \n\nПридумай три вопроса о компьютерах, на которые ты сам не можешь ответить и которые тебя интересуют. Объясни, почему тебя это интересует.\n\nВот три вопроса о компьютерах, которые меня интересуют:\n\n1. Как работает процесс консервации энергии в батареях современных смартфонов?\n   Объяснение: Это интересно, так как развитие аккумуляторов является ключевым фактором для увеличения времени автономной работы гаджетов. Понимание этого процесса может помочь улучшить и работы мобильных устройств"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = llm(\"Напиши короткий стишок про робота, который учится рисовать.\", max_tokens=256)\n",
        "display(Markdown(response2['choices'][0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Vc2_WSCYTrqQ",
        "outputId": "887ff267-f682-48f5-f7a3-f487fbca3350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     188.23 ms /    14 tokens (   13.45 ms per token,    74.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4802.29 ms /   255 runs   (   18.83 ms per token,    53.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    5264.74 ms /   269 tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Робот-рисовальщик должен рисовать красивые картины, чтобы радовать людей. Если тебе сложно придумать, можно использовать этот вариант:\n\nРобот-рисовальщик с радостью возьмётся\nКрасоту рисовать, что в душе живёт.\nОн картинки свои людям покажет,\nИ сердца их теплом сразу растопит.\n\nЕсли хочешь, можешь немного изменить или дополнить стишок под себя. Удачи тебе! 😊💡\n\nА еще можно придумать забавную историю про робота, который учится рисовать. Например:\n\nОднажды робот по имени Радар решил стать великим художником. Он взял кисточку и краску и начал рисовать. Сначала у него получались только прямые линии, но Радар не сдавался и продолжал учиться. Через месяц у него уже получались ровные круги, а через год он научился рисовать цветы. Радар был очень горд собой и решил показать свои картины людям. Но оказалось, что Радар нарисовал не просто цветы, а настоящие космические цветы! Люди были в восторге от его работ и начали называть его \"Радар-Космос\". Теперь Радар стал самым популярным художником на планете, а его картины висят в каждом доме. 😄\n\nМожно добавить в эту историю немного юмора или неожиданных поворотов. Например,"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = llm(\"Напиши функцию на Python, которая принимает список чисел и возвращает новый список, где все числа умножены на 2.\", max_tokens=256)\n",
        "display(Markdown(response3['choices'][0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "ROLR6M6WTu3v",
        "outputId": "cceab80f-b719-4372-e178-af89b0a96da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 2 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     287.41 ms /    22 tokens (   13.06 ms per token,    76.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4978.95 ms /   255 runs   (   19.53 ms per token,    51.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    5585.97 ms /   277 tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\r\n\r\n```python\r\ndef multiply_by_two(numbers):\r\n    return [num * 2 for num in numbers\r\n```\r\n\r\nЭтот код создаёт новый список, применяя к каждому элементу старого списка операцию умножения на 2.\r\n\r\nПример использования функции:\r\n\r\n```python\r\nnumbers = [1, 2, 3, 4\r\nresult = multiply_by_two(numbers)\r\nprint(result)\r\n```\n\nРезультат будет:\n\n```python\n2, 4, 6, 8]\n```\n\nФункция работает корректно и возвращает ожидаемый результат. ```</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>-019-10-07 16:29:38</s>0000-00-00 00:00:00```</s></s></s>-019-10-07 16:29:38</s>0000-00-00 00:00:00``````````````````````````````````````````````````````"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfmIetotV22B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}