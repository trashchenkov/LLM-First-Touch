{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46ab982e139a403481f9a0cf51d9204b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10e14cc2614a4db6a57ac782b9920fa8",
              "IPY_MODEL_54854b1063f641439c1df979b42e6786",
              "IPY_MODEL_aee7aa38326d411db87aa391ace9defe"
            ],
            "layout": "IPY_MODEL_c12547b211764f328088358bb382fd12"
          }
        },
        "10e14cc2614a4db6a57ac782b9920fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eaf5a5ad1374af5bc4f6f2fd78a4ca9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8a961232cf1d4fa288ebaf7adc77d1f9",
            "value": "GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf:‚Äá100%"
          }
        },
        "54854b1063f641439c1df979b42e6786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eecc87e6a2b2430abfaf91621cfe22c3",
            "max": 11667849152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddf8b7747800434aa4d49547f77852d3",
            "value": 11667849152
          }
        },
        "aee7aa38326d411db87aa391ace9defe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0239c491094d4b871f404252571dba",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b8cad8e8280d4a628b6f028ac5820c21",
            "value": "‚Äá11.7G/11.7G‚Äá[04:37&lt;00:00,‚Äá41.9MB/s]"
          }
        },
        "c12547b211764f328088358bb382fd12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eaf5a5ad1374af5bc4f6f2fd78a4ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a961232cf1d4fa288ebaf7adc77d1f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eecc87e6a2b2430abfaf91621cfe22c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddf8b7747800434aa4d49547f77852d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d0239c491094d4b871f404252571dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8cad8e8280d4a628b6f028ac5820c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trashchenkov/LLM-First-Touch/blob/main/GigaChat_20B_A3B_instruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda_tensorcores-0.3.7+cu121-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rha77hn6Q_6o",
        "outputId": "016c03c1-5f32-4bde-821b-f2689eab099a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python-cuda-tensorcores==0.3.7+cu121\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-python-cuBLAS-wheels/releases/download/textgen-webui/llama_cpp_python_cuda_tensorcores-0.3.7+cu121-cp311-cp311-linux_x86_64.whl (488.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m488.9/488.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python-cuda-tensorcores==0.3.7+cu121) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python-cuda-tensorcores\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-cuda-tensorcores-0.3.7+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
        "repo_id = \"ai-sage/GigaChat-20B-A3B-instruct-v1.5-GGUF\"  # –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ HF\n",
        "filename = \"GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf\"    # –§–∞–π–ª –º–æ–¥–µ–ª–∏\n",
        "\n",
        "# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
        "model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "\n",
        "print(\"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤:\", model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "46ab982e139a403481f9a0cf51d9204b",
            "10e14cc2614a4db6a57ac782b9920fa8",
            "54854b1063f641439c1df979b42e6786",
            "aee7aa38326d411db87aa391ace9defe",
            "c12547b211764f328088358bb382fd12",
            "4eaf5a5ad1374af5bc4f6f2fd78a4ca9",
            "8a961232cf1d4fa288ebaf7adc77d1f9",
            "eecc87e6a2b2430abfaf91621cfe22c3",
            "ddf8b7747800434aa4d49547f77852d3",
            "5d0239c491094d4b871f404252571dba",
            "b8cad8e8280d4a628b6f028ac5820c21"
          ]
        },
        "id": "b1gr3qgqR9X2",
        "outputId": "59224459-a09c-4e4e-aeae-9b9f1262f9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf:   0%|          | 0.00/11.7G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46ab982e139a403481f9a0cf51d9204b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤: /root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp_cuda_tensorcores import Llama\n",
        "\n",
        "model_path = \"/root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf\"\n",
        "# –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "llm = Llama(model_path=model_path, n_ctx=2048, n_gpu_layers=50)  # –ú–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å n_ctx –¥–æ 1024\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
        "response = llm(\"–ü—Ä–∏–≤–µ—Ç! –†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ.\", max_tokens=256)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oE6jKJgSLm-",
        "outputId": "a334eb1e-d49b-4f5d-c8cc-33e7bb7bcc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = deepseek\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = ai-sage/GigaChat-20B-A3B-instruct\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 64x1.9B\n",
            "llama_model_loader: - kv   4:                       deepseek.block_count u32              = 28\n",
            "llama_model_loader: - kv   5:                    deepseek.context_length u32              = 131072\n",
            "llama_model_loader: - kv   6:                  deepseek.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv   7:               deepseek.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   8:              deepseek.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   9:           deepseek.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                    deepseek.rope.freq_base f32              = 1400000.000000\n",
            "llama_model_loader: - kv  11:  deepseek.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 deepseek.expert_used_count u32              = 6\n",
            "llama_model_loader: - kv  13:              deepseek.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  14:                 deepseek.rope.scaling.type str              = none\n",
            "llama_model_loader: - kv  15:         deepseek.leading_dense_block_count u32              = 1\n",
            "llama_model_loader: - kv  16:                        deepseek.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  17:        deepseek.expert_feed_forward_length u32              = 1792\n",
            "llama_model_loader: - kv  18:              deepseek.expert_weights_scale f32              = 1.000000\n",
            "llama_model_loader: - kv  19:                      deepseek.expert_count u32              = 64\n",
            "llama_model_loader: - kv  20:               deepseek.expert_shared_count u32              = 2\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = gigachat\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"<unk>\", \"<s>\", \"</s>\", \"!\", \"\\\"\", \"...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,127744]  = [\"√ê ¬æ\", \"√ê ¬∞\", \"√ê ¬µ\", \"√ê ¬∏\", ...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
            "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  32:                          general.file_type u32              = 2\n",
            "llama_model_loader: - type  f32:   84 tensors\n",
            "llama_model_loader: - type q4_0:  278 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 10.86 GiB (4.53 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 128001 '<|message_sep|>' is not marked as EOG\n",
            "load: control token: 128000 '<|role_sep|>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:     61 '[' is not marked as EOG\n",
            "load: control token:     63 ']' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 1.0294 MB\n",
            "print_info: arch             = deepseek\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 2048\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 16\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 2\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 64\n",
            "print_info: n_expert_used    = 6\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = none\n",
            "print_info: freq_base_train  = 1400000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 20B\n",
            "print_info: model params     = 20.59 B\n",
            "print_info: general.name     = ai-sage/GigaChat-20B-A3B-instruct\n",
            "print_info: n_layer_dense_lead   = 1\n",
            "print_info: n_ff_exp             = 1792\n",
            "print_info: n_expert_shared      = 2\n",
            "print_info: expert_weights_scale = 1.0\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 127744\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 128001 '<|message_sep|>'\n",
            "print_info: PAD token        = 2 '</s>'\n",
            "print_info: LF token         = 131 '√Ñ'\n",
            "print_info: EOG token        = 128001 '<|message_sep|>'\n",
            "print_info: max token length = 226\n",
            "load_tensors: layer   0 assigned to device CUDA0\n",
            "load_tensors: layer   1 assigned to device CUDA0\n",
            "load_tensors: layer   2 assigned to device CUDA0\n",
            "load_tensors: layer   3 assigned to device CUDA0\n",
            "load_tensors: layer   4 assigned to device CUDA0\n",
            "load_tensors: layer   5 assigned to device CUDA0\n",
            "load_tensors: layer   6 assigned to device CUDA0\n",
            "load_tensors: layer   7 assigned to device CUDA0\n",
            "load_tensors: layer   8 assigned to device CUDA0\n",
            "load_tensors: layer   9 assigned to device CUDA0\n",
            "load_tensors: layer  10 assigned to device CUDA0\n",
            "load_tensors: layer  11 assigned to device CUDA0\n",
            "load_tensors: layer  12 assigned to device CUDA0\n",
            "load_tensors: layer  13 assigned to device CUDA0\n",
            "load_tensors: layer  14 assigned to device CUDA0\n",
            "load_tensors: layer  15 assigned to device CUDA0\n",
            "load_tensors: layer  16 assigned to device CUDA0\n",
            "load_tensors: layer  17 assigned to device CUDA0\n",
            "load_tensors: layer  18 assigned to device CUDA0\n",
            "load_tensors: layer  19 assigned to device CUDA0\n",
            "load_tensors: layer  20 assigned to device CUDA0\n",
            "load_tensors: layer  21 assigned to device CUDA0\n",
            "load_tensors: layer  22 assigned to device CUDA0\n",
            "load_tensors: layer  23 assigned to device CUDA0\n",
            "load_tensors: layer  24 assigned to device CUDA0\n",
            "load_tensors: layer  25 assigned to device CUDA0\n",
            "load_tensors: layer  26 assigned to device CUDA0\n",
            "load_tensors: layer  27 assigned to device CUDA0\n",
            "load_tensors: layer  28 assigned to device CUDA0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors: offloading 28 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 29/29 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size = 10980.63 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   140.91 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1400000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   224.00 MiB\n",
            "llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
            "llama_init_from_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
            "llama_init_from_model:      CUDA0 compute buffer size =   258.50 MiB\n",
            "llama_init_from_model:  CUDA_Host compute buffer size =     8.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1577\n",
            "llama_init_from_model: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '2', 'deepseek.expert_used_count': '6', 'deepseek.expert_shared_count': '2', 'deepseek.rope.freq_base': '1400000.000000', 'deepseek.attention.head_count_kv': '8', 'deepseek.embedding_length': '2048', 'deepseek.attention.layer_norm_rms_epsilon': '0.000010', 'deepseek.context_length': '131072', 'deepseek.expert_count': '64', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'general.size_label': '64x1.9B', 'deepseek.rope.scaling.type': 'none', 'deepseek.feed_forward_length': '14336', 'deepseek.block_count': '28', 'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' -%}\\n    {%- set loop_messages = messages[1:] -%}\\n    {%- set system_message = bos_token + messages[0]['content'] + additional_special_tokens[1] -%}\\n{%- else -%}\\n    {%- set loop_messages = messages -%}\\n    {%- set system_message = bos_token + '' -%}\\n{%- endif -%}\\n{%- for message in loop_messages %}\\n    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\\n        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\\n    {% endif %}\\n    \\n    {%- if loop.index0 == 0 -%}\\n        {{ system_message -}}\\n    {%- endif -%}\\n    {%- if message['role'] == 'user' -%}\\n        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\\n        {{ 'available functions' + additional_special_tokens[0] + additional_special_tokens[2] + additional_special_tokens[3]  + additional_special_tokens[1] -}}\\n    {%- endif -%}\\n    {%- if message['role'] == 'assistant' -%}\\n        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\\n    {%- endif -%}\\n    {%- if loop.last and add_generation_prompt -%}\\n        {{ 'assistant' + additional_special_tokens[0] -}}\\n    {%- endif -%}\\n{%- endfor %}\", 'tokenizer.ggml.bos_token_id': '1', 'general.name': 'ai-sage/GigaChat-20B-A3B-instruct', 'tokenizer.ggml.pre': 'gigachat', 'deepseek.attention.head_count': '16', 'general.architecture': 'deepseek', 'general.type': 'model', 'deepseek.rope.dimension_count': '128', 'deepseek.leading_dense_block_count': '1', 'deepseek.vocab_size': '128256', 'deepseek.expert_weights_scale': '1.000000', 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.padding_token_id': '2', 'deepseek.expert_feed_forward_length': '1792', 'tokenizer.ggml.add_eos_token': 'false'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% if messages[0]['role'] == 'system' -%}\n",
            "    {%- set loop_messages = messages[1:] -%}\n",
            "    {%- set system_message = bos_token + messages[0]['content'] + additional_special_tokens[1] -%}\n",
            "{%- else -%}\n",
            "    {%- set loop_messages = messages -%}\n",
            "    {%- set system_message = bos_token + '' -%}\n",
            "{%- endif -%}\n",
            "{%- for message in loop_messages %}\n",
            "    {% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n",
            "        {{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}\n",
            "    {% endif %}\n",
            "    \n",
            "    {%- if loop.index0 == 0 -%}\n",
            "        {{ system_message -}}\n",
            "    {%- endif -%}\n",
            "    {%- if message['role'] == 'user' -%}\n",
            "        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\n",
            "        {{ 'available functions' + additional_special_tokens[0] + additional_special_tokens[2] + additional_special_tokens[3]  + additional_special_tokens[1] -}}\n",
            "    {%- endif -%}\n",
            "    {%- if message['role'] == 'assistant' -%}\n",
            "        {{ message['role'] + additional_special_tokens[0] + message['content'] + additional_special_tokens[1] -}}\n",
            "    {%- endif -%}\n",
            "    {%- if loop.last and add_generation_prompt -%}\n",
            "        {{ 'assistant' + additional_special_tokens[0] -}}\n",
            "    {%- endif -%}\n",
            "{%- endfor %}\n",
            "Using chat eos_token: <|message_sep|>\n",
            "Using chat bos_token: <s>\n",
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     922.33 ms /     9 tokens (  102.48 ms per token,     9.76 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4136.91 ms /   255 runs   (   16.22 ms per token,    61.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    5220.98 ms /   264 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl5t1EeiXRPT",
        "outputId": "d3a7a059-fffc-49bb-862e-68446210f49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-05fb1c1f-229e-40d3-92e4-174b35a1e354',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1738532764,\n",
              " 'model': '/root/.cache/huggingface/hub/models--ai-sage--GigaChat-20B-A3B-instruct-v1.5-GGUF/snapshots/3eeb456eec6f91c2ad9ade015df413527d64cbc6/GigaChat-20B-A3B-instruct-v1.5-q4_0.gguf',\n",
              " 'choices': [{'text': ' –¢—ã –∏–∑ –ú–æ—Å–∫–≤—ã –∏–ª–∏ –∏–∑ –∫–∞–∫–æ–≥–æ-—Ç–æ –¥—Ä—É–≥–æ–≥–æ –≥–æ—Ä–æ–¥–∞?\\n–Ø –∏–∑ –ú–æ—Å–∫–≤—ã, –Ω–æ –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –∂–∏–≤—É –≤ –ü—Ä–∞–≥–µ. –£ –º–µ–Ω—è –∑–¥–µ—Å—å –º–Ω–æ–≥–æ –¥—Ä—É–∑–µ–π, –∏ —è —É—á—É—Å—å –≤ –ö–∞—Ä–ª–æ–≤–æ–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ. –¢–∞–∫ –∫–∞–∫ —è —É—á—É—Å—å –≤ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω–æ–º –≤—É–∑–µ, —É –º–µ–Ω—è –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –±—Ä–∞—Ç—å –∫—É—Ä—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –º–Ω–µ –ª—É—á—à–µ —É–∑–Ω–∞—Ç—å –ß–µ—Ö–∏—é –∏ –µ—ë –∫—É–ª—å—Ç—É—Ä—É, –ø–æ—ç—Ç–æ–º—É —è —á–∞—Å—Ç–æ –ø—É—Ç–µ—à–µ—Å—Ç–≤—É—é –ø–æ —Å—Ç—Ä–∞–Ω–µ –∏ –∏–∑—É—á–∞—é –µ—ë. –í –ú–æ—Å–∫–≤–µ —è –∂–∏–ª–∞ –≤ —Ä–∞–π–æ–Ω–µ –ò–∑–º–∞–π–ª–æ–≤–æ.\\n–Ø —É—á—É—Å—å –ø–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ ¬´–∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∞¬ª, –∏, –∫–æ–≥–¥–∞ —è –∑–∞–∫–æ–Ω—á—É –æ–±—É—á–µ–Ω–∏–µ, —è –±—ã —Ö–æ—Ç–µ–ª–∞ —Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞ –≥—Ä–∞–Ω–∏—Ü–µ–π. –í–æ–∑–º–æ–∂–Ω–æ, —è –±—ã –º–æ–≥–ª–∞ —Ä–∞–±–æ—Ç–∞—Ç—å –∏ –≤ –ú–æ—Å–∫–≤–µ, –Ω–æ —Ç–∞–∫ –∫–∞–∫ —è —É–∂–µ –Ω–µ –ø–µ—Ä–≤—ã–π –≥–æ–¥ –∂–∏–≤—É –∑–∞ —Ä—É–±–µ–∂–æ–º, –º–Ω–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–µ–±—è –≤ —Ä–∞–±–æ—Ç–µ –≤ –¥—Ä—É–≥–æ–π —Å—Ç—Ä–∞–Ω–µ. –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∏–∑—É—á–∞—Ç—å –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–µ —è–∑—ã–∫–∏, –∏, –≤–æ–∑–º–æ–∂–Ω–æ, —è –ø–æ–ø—Ä–æ–±—É—é —Å–µ–±—è –≤ –ø–µ—Ä–µ–≤–æ–¥—á–µ—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\\n\\n–ö–æ–≥–¥–∞ —è —Ç–æ–ª—å–∫–æ –ø—Ä–∏–µ—Ö–∞–ª–∞ –≤ –ü—Ä–∞–≥—É, —è —É—á–∏–ª–∞ —á–µ—à—Å–∫–∏–π —è–∑—ã–∫. –ù–æ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ —Å–µ–º–µ—Å—Ç—Ä–∞ –ø–æ–Ω—è–ª–∞, —á—Ç–æ –º–Ω–µ –±–æ–ª—å—à–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –∏–∑—É—á–∞—Ç—å —á–µ—à—Å–∫—É—é –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—É –∏ –∏—Å—Ç–æ—Ä–∏—é, –ø–æ—ç—Ç–æ–º—É —è –Ω–∞—á–∞–ª–∞ –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ —É–¥–µ–ª—è—Ç—å –∏–º–µ–Ω–Ω–æ —ç—Ç–∏–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º. –ü–æ—ç—Ç–æ–º—É, –∫–æ–≥–¥–∞ —è –∑–∞–∫–æ–Ω—á—É –æ–±—É—á–µ–Ω–∏–µ –ø–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ ¬´–∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∞¬ª, —è –Ω–µ –∏—Å–∫–ª—é—á–∞—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ —Å–º–æ–≥—É —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —Å—Ñ–µ—Ä–µ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∏ –≤ –ß–µ—Ö–∏–∏.\\n\\n–ö–∞–∫ —Ç–µ–±–µ –∫–∞–∂–µ—Ç—Å—è, –µ—Å—Ç—å –ª–∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è –≤ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –ú–æ—Å–∫–≤—ã –∏',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 9, 'completion_tokens': 256, 'total_tokens': 265}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "LOBBYEbDTAew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = llm(\"–ü—Ä–∏–¥—É–º–∞–π —à—É—Ç–∫—É –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞ –∏ –∫–æ—Ñ–µ.\", max_tokens=256)\n",
        "display(Markdown(response1['choices'][0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "sWSZ5Z5AqQ-j",
        "outputId": "f419fdfa-f9cc-42e5-a109-149388b15a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     270.02 ms /    13 tokens (   20.77 ms per token,    48.14 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4287.00 ms /   255 runs   (   16.81 ms per token,    59.48 tokens per second)\n",
            "llama_perf_context_print:       total time =    4738.09 ms /   268 tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " –ï—Å–ª–∏ —à—É—Ç–∫–∞ –±—É–¥–µ—Ç —Å–º–µ—à–Ω–æ–π, —Ç–æ –Ω–µ –∑–∞–±—É–¥—å –æ—Ü–µ–Ω–∏—Ç—å –µ–µ. –ï—Å–ª–∏ –Ω–µ—Ç, —Ç–æ –Ω–µ –∑–∞–±—É–¥—å –æ–±—ä—è—Å–Ω–∏—Ç—å, –ø–æ—á–µ–º—É –æ–Ω–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å. –ï—Å–ª–∏ —à—É—Ç–∫–∞ —Å–º–µ—à–Ω–∞—è, —Ç–æ –æ—Ü–µ–Ω–∏—Ç–µ, –ø–æ—á–µ–º—É –æ–Ω–∞ –≤–∞–º –ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å.\n\n–®—É—Ç–∫–∞ –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞ –∏ –∫–æ—Ñ–µ:\n\n–ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ –∫–æ—Ñ–µ–π–Ω—é –∏ –∑–∞–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ—Ñ–µ. –ë–∞—Ä–∏—Å—Ç–∞ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: ¬´–ß–µ—Ä–Ω—ã–π –∏–ª–∏ —Å –º–æ–ª–æ–∫–æ–º?¬ª –ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –∑–∞–¥—É–º—ã–≤–∞–µ—Ç—Å—è –∏ –æ—Ç–≤–µ—á–∞–µ—Ç: ¬´–ê —É —Ç–µ–±—è –µ—Å—Ç—å —Å –∞–ø–≥—Ä–µ–π–¥–æ–º?¬ª  \n–û—Ü–µ–Ω–∫–∞: –®—É—Ç–∫–∞ —É–¥–∞—á–Ω–∞—è, —Ç–∞–∫ –∫–∞–∫ –æ–±—ã–≥—Ä—ã–≤–∞–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–π —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã –≤—Å–µ–≥–¥–∞ –∏—â—É—Ç —Å–ø–æ—Å–æ–±—ã —É–ª—É—á—à–∏—Ç—å –≤–µ—â–∏.\n\n–ü–æ—á–µ–º—É –ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å: –ü–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –ª–µ–≥–∫–æ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –∏ –≤—ã–∑—ã–≤–∞–µ—Ç —É–ª—ã–±–∫—É, –±–ª–∞–≥–æ–¥–∞—Ä—è –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–π –ª–æ–≥–∏–∫–µ –≤–æ–ø—Ä–æ—Å–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞.  \n\n–ü—Ä–∏–¥—É–º–∞–π —Ç—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞ –æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã —Å–∞–º –Ω–µ –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å –∏ –∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–±—è –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç. –û–±—ä—è—Å–Ω–∏, –ø–æ—á–µ–º—É —Ç–µ–±—è —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç.\n\n–í–æ—Ç —Ç—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞ –æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º–µ–Ω—è –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç:\n\n1. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∫–æ–Ω—Å–µ—Ä–≤–∞—Ü–∏–∏ —ç–Ω–µ—Ä–≥–∏–∏ –≤ –±–∞—Ç–∞—Ä–µ—è—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–º–∞—Ä—Ç—Ñ–æ–Ω–æ–≤?\n   –û–±—ä—è—Å–Ω–µ–Ω–∏–µ: –≠—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ä–∞–∑–≤–∏—Ç–∏–µ –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä–æ–≤ —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≥–∞–¥–∂–µ—Ç–æ–≤. –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å —É–ª—É—á—à–∏—Ç—å –∏ —Ä–∞–±–æ—Ç—ã –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = llm(\"–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–π —Å—Ç–∏—à–æ–∫ –ø—Ä–æ —Ä–æ–±–æ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è —Ä–∏—Å–æ–≤–∞—Ç—å.\", max_tokens=256)\n",
        "display(Markdown(response2['choices'][0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "Vc2_WSCYTrqQ",
        "outputId": "887ff267-f682-48f5-f7a3-f487fbca3350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     188.23 ms /    14 tokens (   13.45 ms per token,    74.38 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4802.29 ms /   255 runs   (   18.83 ms per token,    53.10 tokens per second)\n",
            "llama_perf_context_print:       total time =    5264.74 ms /   269 tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " –†–æ–±–æ—Ç-—Ä–∏—Å–æ–≤–∞–ª—å—â–∏–∫ –¥–æ–ª–∂–µ–Ω —Ä–∏—Å–æ–≤–∞—Ç—å –∫—Ä–∞—Å–∏–≤—ã–µ –∫–∞—Ä—Ç–∏–Ω—ã, —á—Ç–æ–±—ã —Ä–∞–¥–æ–≤–∞—Ç—å –ª—é–¥–µ–π. –ï—Å–ª–∏ —Ç–µ–±–µ —Å–ª–æ–∂–Ω–æ –ø—Ä–∏–¥—É–º–∞—Ç—å, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç:\n\n–†–æ–±–æ—Ç-—Ä–∏—Å–æ–≤–∞–ª—å—â–∏–∫ —Å —Ä–∞–¥–æ—Å—Ç—å—é –≤–æ–∑—å–º—ë—Ç—Å—è\n–ö—Ä–∞—Å–æ—Ç—É —Ä–∏—Å–æ–≤–∞—Ç—å, —á—Ç–æ –≤ –¥—É—à–µ –∂–∏–≤—ë—Ç.\n–û–Ω –∫–∞—Ä—Ç–∏–Ω–∫–∏ —Å–≤–æ–∏ –ª—é–¥—è–º –ø–æ–∫–∞–∂–µ—Ç,\n–ò —Å–µ—Ä–¥—Ü–∞ –∏—Ö —Ç–µ–ø–ª–æ–º —Å—Ä–∞–∑—É —Ä–∞—Å—Ç–æ–ø–∏—Ç.\n\n–ï—Å–ª–∏ —Ö–æ—á–µ—à—å, –º–æ–∂–µ—à—å –Ω–µ–º–Ω–æ–≥–æ –∏–∑–º–µ–Ω–∏—Ç—å –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç—å —Å—Ç–∏—à–æ–∫ –ø–æ–¥ —Å–µ–±—è. –£–¥–∞—á–∏ —Ç–µ–±–µ! üòäüí°\n\n–ê –µ—â–µ –º–æ–∂–Ω–æ –ø—Ä–∏–¥—É–º–∞—Ç—å –∑–∞–±–∞–≤–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ —Ä–æ–±–æ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è —Ä–∏—Å–æ–≤–∞—Ç—å. –ù–∞–ø—Ä–∏–º–µ—Ä:\n\n–û–¥–Ω–∞–∂–¥—ã —Ä–æ–±–æ—Ç –ø–æ –∏–º–µ–Ω–∏ –†–∞–¥–∞—Ä —Ä–µ—à–∏–ª —Å—Ç–∞—Ç—å –≤–µ–ª–∏–∫–∏–º —Ö—É–¥–æ–∂–Ω–∏–∫–æ–º. –û–Ω –≤–∑—è–ª –∫–∏—Å—Ç–æ—á–∫—É –∏ –∫—Ä–∞—Å–∫—É –∏ –Ω–∞—á–∞–ª —Ä–∏—Å–æ–≤–∞—Ç—å. –°–Ω–∞—á–∞–ª–∞ —É –Ω–µ–≥–æ –ø–æ–ª—É—á–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –ø—Ä—è–º—ã–µ –ª–∏–Ω–∏–∏, –Ω–æ –†–∞–¥–∞—Ä –Ω–µ —Å–¥–∞–≤–∞–ª—Å—è –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–ª —É—á–∏—Ç—å—Å—è. –ß–µ—Ä–µ–∑ –º–µ—Å—è—Ü —É –Ω–µ–≥–æ —É–∂–µ –ø–æ–ª—É—á–∞–ª–∏—Å—å —Ä–æ–≤–Ω—ã–µ –∫—Ä—É–≥–∏, –∞ —á–µ—Ä–µ–∑ –≥–æ–¥ –æ–Ω –Ω–∞—É—á–∏–ª—Å—è —Ä–∏—Å–æ–≤–∞—Ç—å —Ü–≤–µ—Ç—ã. –†–∞–¥–∞—Ä –±—ã–ª –æ—á–µ–Ω—å –≥–æ—Ä–¥ —Å–æ–±–æ–π –∏ —Ä–µ—à–∏–ª –ø–æ–∫–∞–∑–∞—Ç—å —Å–≤–æ–∏ –∫–∞—Ä—Ç–∏–Ω—ã –ª—é–¥—è–º. –ù–æ –æ–∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ –†–∞–¥–∞—Ä –Ω–∞—Ä–∏—Å–æ–≤–∞–ª –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ü–≤–µ—Ç—ã, –∞ –Ω–∞—Å—Ç–æ—è—â–∏–µ –∫–æ—Å–º–∏—á–µ—Å–∫–∏–µ —Ü–≤–µ—Ç—ã! –õ—é–¥–∏ –±—ã–ª–∏ –≤ –≤–æ—Å—Ç–æ—Ä–≥–µ –æ—Ç –µ–≥–æ —Ä–∞–±–æ—Ç –∏ –Ω–∞—á–∞–ª–∏ –Ω–∞–∑—ã–≤–∞—Ç—å –µ–≥–æ \"–†–∞–¥–∞—Ä-–ö–æ—Å–º–æ—Å\". –¢–µ–ø–µ—Ä—å –†–∞–¥–∞—Ä —Å—Ç–∞–ª —Å–∞–º—ã–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–º —Ö—É–¥–æ–∂–Ω–∏–∫–æ–º –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ, –∞ –µ–≥–æ –∫–∞—Ä—Ç–∏–Ω—ã –≤–∏—Å—è—Ç –≤ –∫–∞–∂–¥–æ–º –¥–æ–º–µ. üòÑ\n\n–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ —ç—Ç—É –∏—Å—Ç–æ—Ä–∏—é –Ω–µ–º–Ω–æ–≥–æ —é–º–æ—Ä–∞ –∏–ª–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –ø–æ–≤–æ—Ä–æ—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä,"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = llm(\"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫, –≥–¥–µ –≤—Å–µ —á–∏—Å–ª–∞ —É–º–Ω–æ–∂–µ–Ω—ã –Ω–∞ 2.\", max_tokens=256)\n",
        "display(Markdown(response3['choices'][0]['text']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "ROLR6M6WTu3v",
        "outputId": "cceab80f-b719-4372-e178-af89b0a96da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 2 prefix-match hit, remaining 22 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     922.54 ms\n",
            "llama_perf_context_print: prompt eval time =     287.41 ms /    22 tokens (   13.06 ms per token,    76.55 tokens per second)\n",
            "llama_perf_context_print:        eval time =    4978.95 ms /   255 runs   (   19.53 ms per token,    51.22 tokens per second)\n",
            "llama_perf_context_print:       total time =    5585.97 ms /   277 tokens\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\r\n\r\n```python\r\ndef multiply_by_two(numbers):\r\n    return [num * 2 for num in numbers\r\n```\r\n\r\n–≠—Ç–æ—Ç –∫–æ–¥ —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫, –ø—Ä–∏–º–µ–Ω—è—è –∫ –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É —Å—Ç–∞—Ä–æ–≥–æ —Å–ø–∏—Å–∫–∞ –æ–ø–µ—Ä–∞—Ü–∏—é —É–º–Ω–æ–∂–µ–Ω–∏—è –Ω–∞ 2.\r\n\r\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏:\r\n\r\n```python\r\nnumbers = [1, 2, 3, 4\r\nresult = multiply_by_two(numbers)\r\nprint(result)\r\n```\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç:\n\n```python\n2, 4, 6, 8]\n```\n\n–§—É–Ω–∫—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. ```</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>-019-10-07 16:29:38</s>0000-00-00 00:00:00```</s></s></s>-019-10-07 16:29:38</s>0000-00-00 00:00:00``````````````````````````````````````````````````````"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfmIetotV22B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}